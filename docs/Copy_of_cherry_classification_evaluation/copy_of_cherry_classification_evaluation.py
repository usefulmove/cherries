# -*- coding: utf-8 -*-
"""Copy of cherry_classification_evaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1azi_Qn1-bjjEjhKY1EzlQaaXJOxtfR3u
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

"""# Cherry Evaluation


**Author**: Wes Havener

This uses the data collected on 11/2/2022 and has several models to choose from:

- model trained on 11_2 data using gradient descent optimizer
- model trained on 11_2 data using ADAM optimizer
- previous model (trained using gradient descent optimizer)

## Install necessary software and import python libraries
"""

pip install fiftyone

# License: BSD
# Author: Sasank Chilamkurthy

from __future__ import print_function, division

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler
import torch.backends.cudnn as cudnn
import numpy as np
import torchvision
from torchvision import datasets, models, transforms
import matplotlib.pyplot as plt
import time
import os
import copy

cudnn.benchmark = True
plt.ion()   # interactive mode

import fiftyone as fo
from fiftyone import ViewField as F

"""## Load Data

Upload a presorted data set


"""

!git clone https://github.com/weshavener/cherry_classification.git

# Data augmentation and normalization for training
# Just normalization for validation

# for this evaluation, use the unmodifided training images
data_transforms = {
    'train': transforms.Compose([
        transforms.CenterCrop(128),
        #transforms.RandomAffine(degrees=(0, 20), translate=(0, 0), scale=(0.85, 1.15)),
        #transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'val': transforms.Compose([
        transforms.CenterCrop(128),
        transforms.ToTensor(),
        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
}

data_dir = 'cherry_classification/data'
image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),
                                          data_transforms[x])
                  for x in ['train', 'val']}
dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=200,
                                             shuffle=True, num_workers=2)
              for x in ['train', 'val']}
dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}
class_names = image_datasets['train'].classes

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

"""## Load the model

uncomment the model you want to run here


- model trained on 11_2 data using gradient descent optimizer
- model trained on 11_2 data using ADAM optimizer
- previous model (trained using gradient descent optimizer)


"""

model_path = '/content/cherry_classification/models/classification_11_2022_all_data_adam.pt'
#model_path = '/content/cherry_classification/models/classification_11_2022_all_data_sgd.pt'
#model_path = '/content/cherry_classification/models/classification_old.pt'

weights = torch.load(model_path)

# Here the size of each output sample is set to 2.
# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).

# get a resnet50 model definition
# set the num classes to 2 instead of the default value
model_ft = models.resnet50(pretrained=False)
num_ftrs = model_ft.fc.in_features
model_ft.fc = nn.Linear(num_ftrs, 2)

model_ft.load_state_dict(weights)


model_ft.to(device)
model_ft.eval()

"""It would be nice to view the dataset with fiftyone, but that is still a work in progress right now

## Run data through model
Run the images through the model and get data for the visualization tool fifytone
"""

samples = []

def add_to_fiddy(input, label, ouput, pred, conf, confs, img_path, dataset_type):
    global samples

    print(label, ouput, pred, conf, confs, img_path)
    sample = fo.Sample(filepath=img_path[0],  tags=[dataset_type])

    classes = ['cherry_clean', 'cherry_pit']

    sample["ground_truth"] = fo.Classification(label=classes[label])
    sample["prediction"] = fo.Classification(label=classes[pred], confidence=conf)

    samples.append(sample)


def evaluate_model(model, num_images, dataset_type):

    was_training = model.training
    model.eval()
    images_so_far = 0
    fig = plt.figure()
    m = nn.Softmax(dim=1)

    with torch.no_grad():

        dl = dataloaders[dataset_type]
        for i, (inputs, labels) in enumerate(dl):
            inputs = inputs.to(device)
            labels = labels.to(device)

            batch_len = inputs.size()[0]

            images = dl.dataset.imgs[i*batch_len:(i+1)*batch_len]

            #print(inputs)
            outputs = model(inputs)
            #print('\n\n')
            #print(outputs)
            #print('\n\n')
            confs = m(outputs)
            index, preds = torch.max(confs, 1)



            #print(outputs)

            for j in range(inputs.size()[0]):
                #print(inputs[j], labels[j], outputs[j], preds[j], index[j], confs[j], images[j], dataset_type)
                add_to_fiddy(inputs[j], labels[j], outputs[j], preds[j], index[j], confs[j], images[j], dataset_type)

                images_so_far += 1
                # ax = plt.subplot(num_images//2, 2, images_so_far)
                # ax.axis('off')
                # ax.set_title(f'predicted: {class_names[preds[j]]}')
                # imshow(inputs.cpu().data[j])



                if images_so_far == num_images:
                    model.train(mode=was_training)
                    return
        model.train(mode=was_training)

evaluate_model(model_ft, 10000, dataset_type='val')
evaluate_model(model_ft, 10000, dataset_type='train')

dataset = fo.Dataset() # use this instead of a specific name to gaurentee a unique databse is created
#fo.Dataset("cherry-classification-dataset")

# this is used if a dataset already exists
#dataset = fo.load_dataset("cherry-classification-dataset")
#dataset.clear()

# add samples to dataset
dataset.add_samples(samples)

"""## Visualiza the results"""

#fo.load_dataset()
#fo.close_app()
session = fo.launch_app(dataset)

# Evaluate the objects in the `predictions` field with respect to the
# objects in the `ground_truth` field

validation_view = dataset.match_tags("val")
validation_results = validation_view.evaluate_classifications(
    "prediction",
)

train_view = dataset.match_tags("train")
train_results = train_view.evaluate_classifications(
    "prediction",
)

all_view = dataset.match_tags(["train", "val"])
all_results = all_view.evaluate_classifications(
    "prediction",
)

print('validation set results:')
validation_results.print_report()
print('train set results:')
train_results.print_report()
print('all results:')
all_results.print_report()

"""# validiation data confusion matrix

This is the most relevant matrix


"""

plot_val = validation_results.plot_confusion_matrix(classes=['cherry_clean', 'cherry_pit'])
plot_val.show()

# Connect to session
session.plots.attach(plot_val)

"""### training data confusion matrix

note that for the old model, all data is new and can be considered validation data
"""

plot_train = train_results.plot_confusion_matrix(classes=['cherry_clean', 'cherry_pit'])
plot_train.show()

# Connect to session
session.plots.attach(plot_train)

"""### all data confusion matrix


"""

plot_all = all_results.plot_confusion_matrix(classes=['cherry_clean', 'cherry_pit'])
plot_all.show()

# Connect to session
session.plots.attach(plot_all)

plot_val.freeze()
plot_train.freeze()
plot_all.freeze()

session.view = all_view


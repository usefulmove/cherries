experiment:
  name: resnet50_differential_lr
  description: "ResNet50 with Differential Learning Rates for better fine-tuning"

data:
  root: "../cherry_classification/data"
  batch_size: 32
  num_workers: 4
  input_size: 128
  normalize: false
  augmentation: true

model:
  architecture: resnet50
  pretrained: true
  num_classes: 2
  freeze_backbone: false  # Will use differential LR instead

training:
  epochs: 30
  optimizer: adam
  # NOTE: Differential LR requires custom parameter groups in notebook
  # Layer configuration:
  #   - layer4 (last residual block): 1e-5
  #   - fc (classifier): 1e-3
  #   - other layers: frozen or 1e-6
  learning_rate: 0.0001  # Base LR (will be overridden by parameter groups)
  weight_decay: 0.00001
  use_scheduler: false
  device: cuda
  mixed_precision: false

checkpointing:
  save_every: 5
  save_best: true
  output_dir: "./outputs/resnet50_differential_lr"

logging:
  log_file: "metrics.json"
  print_every: 1
  save_confusion_matrix: true
